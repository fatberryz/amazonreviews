# Overview

This scraping module will be able to scrape the products, reviews and corresponding profiles off Amazon. 
Products are first identified on Amazon (using their unique ASIN values). The module will then collect information about the product and reviews for these products, as well as the reviewer profiles tied to each review.

This project is meant for academic purposes in a form of a proof of concept. We do not intend to publish this in the public domain for other users.


# Installation
1) Download the project into your directory ```git clone xxxxxxxxx```
2) Go into project directory ```cd amazonreviews/```
3) Create a conda environment ```conda create --name scrapevenv```
then activate it using ```activate scrapevenv```
4) Install environment ```conda env create -f environment.yml```. Alternatively, if using venv then ```pip install -r requirements.txt```

# Usage
1) Modify the file in './data/product_asin.csv' by adding Amazon product ASINs. These ASINs will be used to generate the Amazon links to be scraped by the spiders
2) run '''python main.py''' (after activating the scrapevenv)
- by default, it will automatically scrape reviews, products and profiles

# General Information
** Config File ("config.yml") Parameters:
file_path: default --> ./data : file path containing the csv files with the urls to scrape
freq: default --> 15 : Number of pages to crawl before re-running the script to refresh proxies.
output_dir: default --> ./output/raw : output directory for scraped items
log_output: default --> ./output/logs/ : output directory for log files
final_output: default -->  ./output/consolidated : output directory of combined scraped items
tracker_output: default --> ./output/tracker : output directory of json tracker files
num_retry: default --> 2 : Number of retries of scraping outstanding items"

** Default flow when running 'python main.py' (adjust depending on your needs)

Descrption of sequence of functions in main.py:
X) function_name() --> what it does

1) parse_args() -> read in the default arguments for the scraper from 'config.yml'
2) create_urls() -> generate product reviews + product information links to scrape ('./data/scrape_reviews' and './data/scrape_products.csv') 

* Reviews
3) get_reviews()  --> scrape product reviews based on the ASINs given - scrape all reviews and store in './output/raw/reviews' as csv files, 
unsuccessful scrapes will be stored in the outstanding_reviews log file
4) get_outstanding_reviews() --> scrape outstanding reviews from the log file
5) combine_reviews() --> combine outstanding reviews and previously scraped reviews and stores it in './output/consolidated/consolidated_reviews.csv"
TODO: Scrape reviews in batches of default=15 pages (Do in batches so the proxies wont die halfway)

* Products
6) get_products() -> scrape product information based on the ASINs given - scrape all product information and store in './output/raw/products' as csv files, 
unsuccessful scrapes will be stored in the outstanding_products log file
7) get_outstanding_products() -> scrape outstanding products from the log file
8) combine_products() -> combine outstanding products and previously scraped products and stores it in './output/consolidated/consolidated_products.csv"

* Profiles
9) get_profile_urls() -> get deduplicated profile links from the consolidated reviews csv file 
10) get_profiles() -> scrape profiles based on the ASINs given - scrape all profiles and store in './output/raw/profiles' as csv files, 
unsuccessful scrapes will be stored in the outstanding_profiles log file
11) get_outstanding_profiles() -> scrape outstanding profiles from the log file
12) combine_profiles() -> combine outstanding profiles and previously scraped profiles and stores it in './output/consolidated/consolidated_profiles.csv"

* Post-scraping
13) upload_consolidated_csvs() --> upload the consolidated scraped data into the GBQ database table
14) clear_output_folders() --> clear output of the scraping attempts



